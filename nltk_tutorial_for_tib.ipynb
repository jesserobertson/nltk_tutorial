{
 "metadata": {
  "name": "",
  "signature": "sha256:b4155d770a654167a715e08db6dac1f4d2735b1053ab39366c6105577e34e8c3"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Introduction to text analysis in Python"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#### This tutorial provides: \n",
      "\n",
      "1. A very brief introduction to Python programming.\n",
      "2. An introduction to natural language processing with NLTK. \n",
      "3. An example of a measure of text similarity. \n",
      "4. An example of supervised classification using NLTK. \n",
      "5. An example of clustering with NLTK. \n",
      "\n",
      "\n",
      "#### Downloading the notebook\n",
      "\n",
      "Before we begin, you need to **create a free account on www.wakari.io **\n",
      "\n",
      "You can import this notebook directly into Makari by clicking on the globe in the upper left corner (import from web) and entering the following URL: \n",
      "\n",
      "** https://dl.dropboxusercontent.com/u/11117852/Text_tutorial_updated.ipynb **\n",
      "\n",
      "You can also download the notebook and save it on your drive. \n",
      "\n",
      "\n",
      "#### How does the notebook work? \n",
      "\n",
      "The tutorial is running on a program called IPython, or \"InteractivePython\". The IPython Notebook runs in the browser. You won't need to install Python or any of the libraries and modules we will be working with today on your computer. Instead, we are using Wakari, a service that allows you to run IPython notebooks in the cloud.  \n",
      "\n",
      "The notebook is separated by horizontal cells. Some of these cells contain text (instructions, extra information) and others are Python input fields, where you can write and execute code. \n",
      "\n",
      "- To navigate between cells you can click on the cell with your mouse, or use the up and down arrows on your keyboard. \n",
      "- If you double click on a text cell you can edit it (you can add your own notes, for example). \n",
      "- To write code in an input field simply click on the field and start typing. \n",
      "- To execute code in a cell click the \"play\" button in the tool bar above. The output will appear bellow the cell. You can also run the code in a cell by pressing Control-Shift-Enter on a PC or Command-Shift-Return on a Mac.\n",
      "\n",
      "When we finish, you will be able to save all of your work to a Python script file and keep it for your records. \n",
      "\n",
      "#### Resources used\n",
      "Examples based almost entirely on the NLTK manual: \n",
      "**Natural Language Processing with Python- Analyzing Text with the Natural Language Toolkit**\n",
      "by Steven Bird, Ewan Klein and Edward Loper (free online: http://www.nltk.org/book/ )\n"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "1. Intro to Python\n"
     ]
    },
    {
     "cell_type": "raw",
     "metadata": {},
     "source": [
      "In the box below, type:\n",
      "print \"Hello, world!\""
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print 'Hello world!'"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Hello world!\n"
       ]
      }
     ],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "a=1+1\n",
      "a"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 3,
       "text": [
        "2"
       ]
      }
     ],
     "prompt_number": 3
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#### Working with strings of characters (text). \n",
      "\n",
      "Copy and paste the content of the following cells into the input cells below them. Execute the code to see how it works.  "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "monty = \"Monty Python's Flying Circus. \" \n",
      "# Monty is a text variable. And lines which start with a # are comments are are not executed. \n",
      "monty"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 4,
       "text": [
        "\"Monty Python's Flying Circus. \""
       ]
      }
     ],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "monty*2 + \" Plus just the last word:\" + monty[-8:]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 5,
       "text": [
        "\"Monty Python's Flying Circus. Monty Python's Flying Circus.  Plus just the last word:Circus. \""
       ]
      }
     ],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "monty.find('Python') #finds position of substring within string"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 6,
       "text": [
        "6"
       ]
      }
     ],
     "prompt_number": 6
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "monty.upper() +' and '+ monty.lower() # turn to upper or lower case. "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 7,
       "text": [
        "\"MONTY PYTHON'S FLYING CIRCUS.  and monty python's flying circus. \""
       ]
      }
     ],
     "prompt_number": 7
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "monty.replace('y', 'x') # replace letter y in the string with letter x. "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 8,
       "text": [
        "\"Montx Pxthon's Flxing Circus. \""
       ]
      }
     ],
     "prompt_number": 8
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#### Lists\n",
      "As opposed to strings, lists are flexible about the elements they contain. \n",
      "Run the code below. It creates three list variables. \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "list1 = ['Monty', 'Python']\n",
      "list2 = ['and', 'the', 'Holy', 'Grail']\n",
      "list3= [1, 2, 3]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 9
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "len(list2)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 10,
       "text": [
        "4"
       ]
      }
     ],
     "prompt_number": 10
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "list1[1]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 11,
       "text": [
        "'Python'"
       ]
      }
     ],
     "prompt_number": 11
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "list1 + list2 + list3 # adding the three lists together"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 12,
       "text": [
        "['Monty', 'Python', 'and', 'the', 'Holy', 'Grail', 1, 2, 3]"
       ]
      }
     ],
     "prompt_number": 12
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "list2.append(\"1975\") # adding an element to a list\n",
      "list2"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 13,
       "text": [
        "['and', 'the', 'Holy', 'Grail', '1975']"
       ]
      }
     ],
     "prompt_number": 13
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "sorted(list1 + list2) # sorting the elements of the two combined lists"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 14,
       "text": [
        "['1975', 'Grail', 'Holy', 'Monty', 'Python', 'and', 'the']"
       ]
      }
     ],
     "prompt_number": 14
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Join the two string elements of the list by a single space.\n",
      "# The result is a string. \n",
      "' '.join(['Monty', 'Python']) "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 15,
       "text": [
        "'Monty Python'"
       ]
      }
     ],
     "prompt_number": 15
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#### Regular exppressions. \n",
      "\n",
      "Huge topic. You will need to learn more about regular expressions if you plan on working regularly with text. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import re"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 16
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "\n",
      "Find and count all vowels.  "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "word = 'supercalifragilisticexpialidocious'\n",
      "len(re.findall(r'[aeiou]', word))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 17,
       "text": [
        "16"
       ]
      }
     ],
     "prompt_number": 17
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "2. NLP with NLTK\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#### Import NLTK corpora\n",
      "\n",
      "Import text4 of NLTK book examples, Inaugural Addresses. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import nltk"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 18
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "** CAREFUL: **  The command bellow opens a downloader. \n",
      "\n",
      "In the bottom row, when prompted, you need to type: **d book**\n",
      "\n",
      "Then let it run, it might take some time. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "nltk.download()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "NLTK Downloader\n",
        "---------------------------------------------------------------------------\n",
        "    d) Download   l) List    u) Update   c) Config   h) Help   q) Quit\n",
        "---------------------------------------------------------------------------\n"
       ]
      },
      {
       "name": "stdout",
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Downloader> d book\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "    Downloading collection 'book'\n",
        "       | \n",
        "       | Downloading package 'abc' to\n",
        "       |     /user_home/w_jessrobertson/nltk_data...\n",
        "       |   Unzipping corpora/abc.zip."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "       | Downloading package 'brown' to\n",
        "       |     /user_home/w_jessrobertson/nltk_data..."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "       |   Unzipping corpora/brown.zip."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "       | Downloading package 'chat80' to\n",
        "       |     /user_home/w_jessrobertson/nltk_data..."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "       |   Unzipping corpora/chat80.zip."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "       | Downloading package 'cmudict' to\n",
        "       |     /user_home/w_jessrobertson/nltk_data..."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "       |   Unzipping corpora/cmudict.zip."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "       | Downloading package 'conll2000' to\n",
        "       |     /user_home/w_jessrobertson/nltk_data..."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "       |   Unzipping corpora/conll2000.zip."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "       | Downloading package 'conll2002' to\n",
        "       |     /user_home/w_jessrobertson/nltk_data..."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "       |   Unzipping corpora/conll2002.zip."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "       | Downloading package 'dependency_treebank' to\n",
        "       |     /user_home/w_jessrobertson/nltk_data..."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "       |   Unzipping corpora/dependency_treebank.zip."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "       | Downloading package 'genesis' to\n",
        "       |     /user_home/w_jessrobertson/nltk_data..."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "       |   Unzipping corpora/genesis.zip."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "       | Downloading package 'gutenberg' to\n",
        "       |     /user_home/w_jessrobertson/nltk_data..."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "       |   Unzipping corpora/gutenberg.zip."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "       | Downloading package 'ieer' to\n",
        "       |     /user_home/w_jessrobertson/nltk_data..."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "       |   Unzipping corpora/ieer.zip."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "       | Downloading package 'inaugural' to\n",
        "       |     /user_home/w_jessrobertson/nltk_data..."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "       |   Unzipping corpora/inaugural.zip."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "       | Downloading package 'movie_reviews' to\n",
        "       |     /user_home/w_jessrobertson/nltk_data..."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "       |   Unzipping corpora/movie_reviews.zip."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "       | Downloading package 'nps_chat' to\n",
        "       |     /user_home/w_jessrobertson/nltk_data..."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "       |   Unzipping corpora/nps_chat.zip."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "       | Downloading package 'names' to\n",
        "       |     /user_home/w_jessrobertson/nltk_data..."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "       |   Unzipping corpora/names.zip."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "       | Downloading package 'ppattach' to\n",
        "       |     /user_home/w_jessrobertson/nltk_data...\n",
        "       |   Unzipping corpora/ppattach.zip."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "       | Downloading package 'reuters' to\n",
        "       |     /user_home/w_jessrobertson/nltk_data..."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "       | Downloading package 'senseval' to\n",
        "       |     /user_home/w_jessrobertson/nltk_data..."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "       |   Unzipping corpora/senseval.zip."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "       | Downloading package 'state_union' to\n",
        "       |     /user_home/w_jessrobertson/nltk_data..."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "       |   Unzipping corpora/state_union.zip."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "       | Downloading package 'stopwords' to\n",
        "       |     /user_home/w_jessrobertson/nltk_data..."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "       |   Unzipping corpora/stopwords.zip."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "       | Downloading package 'swadesh' to\n",
        "       |     /user_home/w_jessrobertson/nltk_data..."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "       |   Unzipping corpora/swadesh.zip."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "       | Downloading package 'timit' to\n",
        "       |     /user_home/w_jessrobertson/nltk_data..."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "       |   Unzipping corpora/timit.zip."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "       | Downloading package 'treebank' to\n",
        "       |     /user_home/w_jessrobertson/nltk_data..."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "       |   Unzipping corpora/treebank.zip."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "       | Downloading package 'toolbox' to\n",
        "       |     /user_home/w_jessrobertson/nltk_data..."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "       |   Unzipping corpora/toolbox.zip."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "       | Downloading package 'udhr' to\n",
        "       |     /user_home/w_jessrobertson/nltk_data..."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "       |   Unzipping corpora/udhr.zip."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "       | Downloading package 'udhr2' to\n",
        "       |     /user_home/w_jessrobertson/nltk_data..."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "       |   Unzipping corpora/udhr2.zip."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "       | Downloading package 'unicode_samples' to\n",
        "       |     /user_home/w_jessrobertson/nltk_data..."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "       |   Unzipping corpora/unicode_samples.zip."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "       | Downloading package 'webtext' to\n",
        "       |     /user_home/w_jessrobertson/nltk_data...\n",
        "       |   Unzipping corpora/webtext.zip."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "       | Downloading package 'wordnet' to\n",
        "       |     /user_home/w_jessrobertson/nltk_data..."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "       |   Unzipping corpora/wordnet.zip."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "       | Downloading package 'wordnet_ic' to\n",
        "       |     /user_home/w_jessrobertson/nltk_data..."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "       |   Unzipping corpora/wordnet_ic.zip."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "       | Downloading package 'words' to\n",
        "       |     /user_home/w_jessrobertson/nltk_data..."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "       |   Unzipping corpora/words.zip."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "       | Downloading package 'maxent_treebank_pos_tagger' to\n",
        "       |     /user_home/w_jessrobertson/nltk_data..."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "       |   Unzipping taggers/maxent_treebank_pos_tagger.zip."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "       | Downloading package 'maxent_ne_chunker' to\n",
        "       |     /user_home/w_jessrobertson/nltk_data..."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "       |   Unzipping chunkers/maxent_ne_chunker.zip."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "       | Downloading package 'universal_tagset' to\n",
        "       |     /user_home/w_jessrobertson/nltk_data..."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "       |   Unzipping taggers/universal_tagset.zip."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "       | Downloading package 'punkt' to\n",
        "       |     /user_home/w_jessrobertson/nltk_data..."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "       |   Unzipping tokenizers/punkt.zip."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "       | Downloading package 'book_grammars' to\n",
        "       |     /user_home/w_jessrobertson/nltk_data..."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "       |   Unzipping grammars/book_grammars.zip."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "       | Downloading package 'city_database' to\n",
        "       |     /user_home/w_jessrobertson/nltk_data..."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "       |   Unzipping corpora/city_database.zip."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "       | Downloading package 'tagsets' to\n",
        "       |     /user_home/w_jessrobertson/nltk_data...\n",
        "       |   Unzipping help/tagsets.zip."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "       | Downloading package 'panlex_swadesh' to\n",
        "       |     /user_home/w_jessrobertson/nltk_data..."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "       | Downloading package 'averaged_perceptron_tagger' to\n",
        "       |     /user_home/w_jessrobertson/nltk_data..."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "       |   Unzipping taggers/averaged_perceptron_tagger.zip."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "       | "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "     Done downloading collection 'book'\n",
        "\n",
        "---------------------------------------------------------------------------\n",
        "    d) Download   l) List    u) Update   c) Config   h) Help   q) Quit\n",
        "---------------------------------------------------------------------------\n"
       ]
      },
      {
       "name": "stdout",
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Downloader> q\n"
       ]
      },
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 19,
       "text": [
        "True"
       ]
      }
     ],
     "prompt_number": 19
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "When done, to exit the downloader type: **q**\n",
      "\n",
      "We will now import a text in the Inaugural Addresses collection: "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from nltk.book import text4"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#### Operating on every element. List comprehension.\n",
      "Read the code below, think about what it is supposed to do, then run it to see what it does. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "len(set([word.lower() for word in text4 if len(word)>5]))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "[element.upper() for element in text4[0:5]]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "for word in text4[0:5]:\n",
      "    if len(word)<5 and word.endswith('e'):\n",
      "        print word, ' is short and ends with e '\n",
      "    elif word.istitle():\n",
      "        print word, ' is a titlecase word '\n",
      "    else:\n",
      "        print word, ' is just another word '"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**Exercise:** Search within the first 100 words of text4 and display those that are longer that 8 letters and end in a \"g\". "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#### Words in context\n",
      "\n",
      "Search word in text, diasplay the results together with the context:\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "text4.concordance(\"America\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "What other words appear in a similar range of contexts? "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "text4.similar(\"citizen\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Examine just the contexts that are shared by two or more words:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "text4.common_contexts([\"war\", \"freedom\"])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Location of a word in the text: how many spaces from the beginning does it appear? \n",
      "\n",
      "This positional information can be displayed using a dispersion plot. \n",
      "\n",
      "You need NumPy and Matplotlib. \n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Start pylab inline mode, so figures will appear in the notebook\n",
      "%pylab inline"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy, matplotlib\n",
      "from nltk.draw.dispersion import dispersion_plot\n",
      "dispersion_plot(text4, [\"citizens\", \"democracy\", \"freedom\", \"war\", \"America\", \"vote\"])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#### Counting\n",
      "The length of a text from start to finish, in terms of the words and punctuation symbols that appear. All tokens. \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "len(text4)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Count how often a word occurs in a text:\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "text4.count(\"democracy\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "How many distinct words does the book of Genesis contain? \n",
      "The vocabulary of a text is just the set of tokens that it uses. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "len(set(text4)) #types\n",
      "# Each word used on average x times. Richness of the text. \n",
      "len(text4) / len(set(text4)) "
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#### Define functions: \n",
      "\n",
      "What do you think they do? \n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def lexical_diversity(text):\n",
      "    return len(set(text))/len(text)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def percentage(count, total):\n",
      "    return count/total"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Then use the defined functions:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from __future__ import division #to get precise (float) division in Python 2.x. In Python 3.0 you get it automatically. \n",
      "lexical_diversity(text4)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "percentage(text4.count('the'), len(text4)) "
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#### Simple statistics\n",
      "\n",
      "Counting Words Appearing in a Text (a frequency distribution). \n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from nltk import FreqDist\n",
      "fdist1 = FreqDist(text4)\n",
      "fdist1"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "vocabulary1 = fdist1.keys() # list of all the distinct types in the text\n",
      "vocabulary1[:3] # look at first 3"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "- words that occur only once, called hapaxes: "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "fdist1.hapaxes()[:20]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      " \n",
      "- words that meet a condition, are long for example\n",
      "    "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "V = set(text4)\n",
      "long_words = [w for w in V if len(w) > 20]\n",
      "sorted(long_words)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "- words that characterize a text (are relatively long, and occur frequently)"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "fdist = FreqDist(text4)\n",
      "sorted([w for w in set(text4) if len(w) > 12 and fdist[w] > 7])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#### Conditional frequency distributions\n",
      "\n",
      "Working with the inaugural corpus:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from nltk.corpus import inaugural\n",
      "inaugural.fileids()[:2]\n",
      "[fileid[:4] for fileid in inaugural.fileids()] # Get the first 4 characters of the file IDs"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "How are the words \"America\" and \"citizen\" are used over time?"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "cfd = nltk.ConditionalFreqDist(\n",
      "    (target, fileid[:4])\n",
      "    for fileid in inaugural.fileids()\n",
      "    for w in inaugural.words(fileid)\n",
      "    for target in ['america', 'war']\n",
      "    if w.lower().startswith(target))\n",
      "cfd.plot()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Working with the news database corpus:\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from nltk.corpus import brown\n",
      "news_words=brown.words(categories=\"news\") \n",
      "print(news_words) # get the first words in the corpus"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "freq= nltk.FreqDist(news_words)\n",
      "freq.plot(30) # frequency of most commonly used words in the corpus"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "How are different verbs used in different news genres? "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from nltk import FreqDist\n",
      "verbs=[\"should\", \"may\", \"can\"]\n",
      "genres=[\"news\", \"government\", \"romance\"]\n",
      "for g in genres:\n",
      "    words=brown.words(categories=g)\n",
      "    freq=FreqDist([w.lower() for w in words if w.lower() in verbs])\n",
      "    print g, freq\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#### Stopwords\n",
      "\n",
      "What percentage of the words in a corpus are NOT stopwords? \n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from nltk.corpus import stopwords\n",
      "stopwords.words('english')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def content_fraction(text):\n",
      "    stopwords = nltk.corpus.stopwords.words('english')\n",
      "    content = [w for w in text if w.lower() not in stopwords]\n",
      "    return len(content) / len(text)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print content_fraction(nltk.corpus.inaugural.words())"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Importing and accessing your own text"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Useful libraries: "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import nltk, re, pprint\n",
      "from urllib import urlopen"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### User input\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "s = raw_input(\"Enter some text: \")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Online articles\n",
      "Getting text out of HTML is a sufficiently common task that NLTK provides a helper function nltk.clean_html(), which takes an HTML string and returns raw text."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "url = \"http://www.bbc.co.uk/news/education-24367153\"\n",
      "html = urlopen(url).read()\n",
      "raw = nltk.clean_html(html)  \n",
      "raw[:60]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "raw = nltk.clean_html(html)\n",
      "tokens = nltk.word_tokenize(raw)\n",
      "tokens[:15]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Online books"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "url=\"http://shakespeare.mit.edu/hamlet/full.html\"\n",
      "html = urlopen(url).read()    \n",
      "raw = nltk.clean_html(html)  \n",
      "print raw[:300]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "tokens = nltk.word_tokenize(raw)\n",
      "type(tokens)\n",
      "tokens[50:70]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**Exercise:** Find out the type of variable tokens, the length and display tokens from 80 to 100. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "text = nltk.Text(tokens)\n",
      "text.collocations()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Local files\n",
      "Using local files. \n",
      "Upload the following file to Wakari, by clicking the 'Import from web' icon in the upper left side corner:  https://dl.dropboxusercontent.com/u/11117852/UK_natl_2010_en_Lab.txt. Now the file is saved in your account and you can use it in the analysis. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "f = open(\"UK_natl_2010_en_Lab.txt\", 'r')\n",
      "raw = f.read()\n",
      "print raw[:100]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Tokenize - divide into tokens:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "tokens = nltk.word_tokenize(raw)\n",
      "tokens[:10]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      " Normalize - ignore upper case"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "lower_case=set(w.lower() for w in tokens)\n",
      "print len(lower_case)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Stemming - strip off affixes"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "porter = nltk.PorterStemmer()\n",
      "a=[porter.stem(t) for t in tokens]\n",
      "a[60:90]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "lancaster = nltk.LancasterStemmer()\n",
      "a=[lancaster.stem(t) for t in tokens]\n",
      "a[60:90]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Lemmatizing - the word is from a dictionary"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "wnl = nltk.WordNetLemmatizer()\n",
      "a=[wnl.lemmatize(t) for t in tokens]\n",
      "a[60:90]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Sentence segmentation:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "sent_tokenizer=nltk.data.load('tokenizers/punkt/english.pickle')\n",
      "sents = sent_tokenizer.tokenize(raw)\n",
      "pprint.pprint(sents[182:185])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Writing output to file. The file is created in your Wakari directory. Here, we are writing each sentence on a separate line. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "output_file = open('output.txt', 'w')\n",
      "sentence = set(sents)\n",
      "for sent in sorted(sentence):\n",
      "   output_file.write(sent + \"\\n\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "raw",
     "metadata": {},
     "source": [
      "Hit the refresh button in the upper left side corner to view an updated list that includes the newly created file. Have a look at it. "
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Text similarity"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We can use both NLTK and scikit-learn for this. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.feature_extraction.text import TfidfVectorizer"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Calculate tf-idf:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "vect = TfidfVectorizer(min_df=1)\n",
      "tfidf = vect.fit_transform([\"New Year's Eve in New York\",\n",
      "                            \"New Year's Eve in London\",\n",
      "                            \"York is closer to London than to New York\",\n",
      "                            \"London is closer to Bucharest than to New York\"])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Calculate cosine similarity:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "cosine=(tfidf * tfidf.T).A\n",
      "print cosine"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Trained classification with NLTK"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#### Names-gender identification example"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from nltk.corpus import names\n",
      "import random"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Select relevant fearures. Here, last letter of name. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def gender_features(word):\n",
      "    return {'last_letter': word[-1]}"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "What is the feature for the name Shrek? \n",
      "\n",
      "gender_features('Shrek')\n",
      "\n",
      "What is the feature for your own name? "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "gender_features('iulia')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Train and test data: "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "names = ([(name, 'male') for name in names.words('male.txt')] +\n",
      "          [(name, 'female') for name in names.words('female.txt')])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Arrange data randomly and extract features"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "random.shuffle(names)\n",
      "featuresets = [(gender_features(n), g) for (n,g) in names]\n",
      "from nltk.classify import apply_features # use apply if you're working with large corpora"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Divide data into training and test sets:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "train_set = apply_features(gender_features, names[500:1000])\n",
      "test_set = apply_features(gender_features, names[:500])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Use a Naive Bayes Classifier:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "classifier = nltk.NaiveBayesClassifier.train(train_set)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Classify the test set and evaluate performance"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print nltk.classify.accuracy(classifier, test_set)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "What are the most informative features?"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "classifier.show_most_informative_features(5)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "Use the algorithm to classify new data:"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "classifier.classify(gender_features('iulia'))\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "classifier.classify(gender_features('cioroianu'))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#### Exercise: \n",
      "    What other features could be relevant?\n",
      "    Repeat the classification with the first letter of the name as the relevant feature. \n",
      "    Compare the accuracy and the most informative features. \n",
      "    Test it on your first and middle names. \n",
      "    Write all your code in the cell below. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Clustering"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The example below is based on this one: https://gist.github.com/xim/1279283 (by Morten Neergaard)"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy\n",
      "from nltk.cluster import KMeansClusterer, GAAClusterer, euclidean_distance\n",
      "import nltk.corpus\n",
      "import nltk.stem\n",
      "stemmer_func = nltk.stem.snowball.SnowballStemmer(\"english\").stem\n",
      "stopwords = set(nltk.corpus.stopwords.words('english'))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Define normalize function"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def normalize_word(word):\n",
      "    return stemmer_func(word.lower())"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Define feature selection function"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def get_words(titles):\n",
      "    words = set()\n",
      "    for title in job_titles:\n",
      "        for word in title.split():\n",
      "            words.add(normalize_word(word))\n",
      "    return list(words)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Define vector space function"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def vectorspaced(title):\n",
      "    title_components = [normalize_word(word) for word in title.split()]\n",
      "    return numpy.array([\n",
      "        word in title_components and not word in stopwords\n",
      "        for word in words], numpy.short)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Upload example file. The file is here: https://dl.dropboxusercontent.com/u/11117852/example_jobs.txt . "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "title_file = open(\"example_jobs.txt\", 'r')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Get features"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "job_titles = [line.strip() for line in title_file.readlines()]\n",
      "words = get_words(job_titles)\n",
      "words[0:10]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "K-Means clustering: "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "cluster = KMeansClusterer(7, euclidean_distance)\n",
      "cluster.cluster([vectorspaced(title) for title in job_titles if title])\n",
      "classified_examples = [cluster.classify(vectorspaced(title)) for title in job_titles]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Print results:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "for cluster_id, title in sorted(zip(classified_examples, job_titles)):\n",
      "    print cluster_id, title"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "\n",
      "\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**Exercise:** Modify the number of clusters and see how the results change. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**Exercise:** Modify the script above to implement group-average agglomerative clustering, with n classes, instead of K-means clustering. The corresponding NLTK function is: GAAClusterer(n).  "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}